{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# mBERT for topic labeling\n",
    "\n",
    "**Dependencies**\n",
    "- tokenizers\n",
    "- torch\n",
    "- transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load our model that was pretrained on this task. We also load in the tokenizer.\n",
    "\n",
    "Because we only want to get results, we have to disable dropout etc. So we add `model.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DTAI-KULeuven/mbert-corona-tweets-belgium-topics'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path=\"DTAI-KULeuven/mbert-corona-tweets-belgium-topics\"\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0f83e3c2494b9c9b766b2166377225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1227.0, style=ProgressStyle(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac24f2ea9eb4412b302876c391e5518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd71e542a992461582d4f157fd64e4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=312.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e469fc61ce485f8d1a2322a7059c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=711499087.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, return_dict=True)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "## Sanity check\n",
    "Let's first check if the predictions make sense, just to make sure we loaded the model correctly :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "\ttensor([[  101, 49082, 28883,   131, 82386, 10423, 41208, 13173, 74755, 29797,\n",
      "         11062,   131, 16527, 10410, 11422, 10104, 82991,   106,   100,   118,\n",
      "           118,   118,   118,   118,   118,   118,   118,   118,   118, 61444,\n",
      "         10123,   131, 56898,   137, 14268, 12955, 11273, 22659, 10157, 10759,\n",
      "           131, 82386, 10423, 41208, 13173, 74755, 29797, 11062,   131, 16527,\n",
      "         10410, 11422, 10104, 82991,   106,   100,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 49082, 28883,   131, 35936,   137, 11806, 15417, 11008, 22572,\n",
      "         10110,   137, 10104,   168,   151, 47172, 10187, 10124, 11810, 10527,\n",
      "         23005, 14088, 10112, 10187, 81477, 11810, 47458, 10168, 12620, 15210,\n",
      "         88429, 10112, 34657, 11534, 67875, 10174, 10106, 42617,   119, 10915,\n",
      "         10124, 10164, 10200, 19160, 12044, 52280, 15703, 12563, 70058,   119,\n",
      "           118,   118,   118,   118,   118,   118,   118,   118,   118,   118,\n",
      "         61444, 10123,   131, 56898,   137, 24560, 11369, 79229, 10107,   131,\n",
      "         35936,   137, 11806, 15417, 11008, 22572, 10110,   137, 10104,   168,\n",
      "           151, 47172, 10187, 10124, 11810, 10527, 23005, 14088, 10112, 10187,\n",
      "         81477, 11810, 47458, 10168, 12620, 15210, 88429, 10112, 34657, 11534,\n",
      "         67875, 10174, 10106, 42617,   119, 10915, 10124, 10164,   100,   102],\n",
      "        [  101, 10747, 18322, 10472, 10347,   169, 12541, 10550,   106, 11966,\n",
      "         10187, 15210, 34657, 11534, 67875, 10174, 10134, 13629, 47458, 11085,\n",
      "         12555, 23653,   117, 10257, 10457, 10730, 10350, 12713, 33423, 54242,\n",
      "         81477, 10527, 25569, 14302, 10333, 10428, 10144, 10124,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "token_type_ids:\n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask:\n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Tokens:\n",
      "\t['[CLS]', 'Conte', '##xt', ':', 'Tip', 'voor', 'onze', 'ho', '##rec', '##aza', '##ken', ':', 'sta', '##p', 'naar', 'de', 'rechter', '!', '[UNK]', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Twee', '##t', ':', 'RT', '@', 'Sam', '##van', '##R', '##oo', '##y', '##1', ':', 'Tip', 'voor', 'onze', 'ho', '##rec', '##aza', '##ken', ':', 'sta', '##p', 'naar', 'de', 'rechter', '!', '[UNK]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\t['[CLS]', 'Conte', '##xt', ':', 'Hey', '@', 'Jan', '##J', '##am', '##bon', 'en', '@', 'de', '_', 'N', '##VA', 'het', 'is', 'maar', 'dat', 'ju', '##lli', '##e', 'het', 'weten', 'maar', 'ik', 'du', '##ld', 'geen', 'vroeger', '##e', 'avo', '##nd', '##klo', '##k', 'in', 'Vlaanderen', '.', 'Er', 'is', 'al', 'een', 'samen', '##sch', '##olin', '##gs', '##ver', '##bod', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Twee', '##t', ':', 'RT', '@', 'Karen', '##L', '##iesen', '##s', ':', 'Hey', '@', 'Jan', '##J', '##am', '##bon', 'en', '@', 'de', '_', 'N', '##VA', 'het', 'is', 'maar', 'dat', 'ju', '##lli', '##e', 'het', 'weten', 'maar', 'ik', 'du', '##ld', 'geen', 'vroeger', '##e', 'avo', '##nd', '##klo', '##k', 'in', 'Vlaanderen', '.', 'Er', 'is', 'al', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.batch_encode_plus(\n",
    "    [\"Context: Tip voor onze horecazaken: stap naar de rechter! ðŸ‘‡  ---------- Tweet: RT @SamvanRooy1: Tip voor onze horecazaken: stap naar de rechter! ðŸ‘‡\",\n",
    "     \"Context: Hey @JanJambon en @de_NVA het is maar dat jullie het weten  maar ik duld geen vroegere avondklok in Vlaanderen. Er is al een samenscholingsverbod. ---------- Tweet: RT @KarenLiesens: Hey @JanJambon en @de_NVA het is maar dat jullie het weten  maar ik duld geen vroegere avondklok in Vlaanderen. Er is alâ€¦\",\n",
    "     \"This better not be a joke! Als het geen avondklok was ging ik nu zoeken, aub kacper laat ons weten dat alles okÃ© met je is\"],\n",
    "    return_tensors=\"pt\", padding=True)\n",
    "for key, value in inputs.items():\n",
    "    inputs[key] = value.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"{}:\\n\\t{}\".format(key, value))\n",
    "print(\"Tokens:\\n\\t{}\".format(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]) ))\n",
    "print(\"\\t{}\".format(tokenizer.convert_ids_to_tokens(inputs['input_ids'][1]) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In our model config, we stored what labels we use (`0 = joke` and `1 = proverb`). We can load these in and automatically convert our predictions to a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'other-measure', 1: 'closing-horeca', 2: 'testing', 3: 'schools', 4: 'lockdown', 5: 'quarantine', 6: 'curfew', 7: 'masks', 8: 'not-applicable', 9: 'vaccine'}\n"
     ]
    }
   ],
   "source": [
    "for i in model.config.id2label:\n",
    "    model.config.id2label[i] = model.config.id2label[i].replace(\"\\n\",\"\")\n",
    "    \n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's do some predictions! Since we have a batch of 2 jokes and one proverb, we can do this in one batchâ€”as long as it fits on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['closing-horeca', 'curfew', 'curfew']\n",
      "tensor([[ 0.0199,  2.3763, -1.3540, -0.0728,  1.7234, -1.0660, -1.9340, -1.4213,\n",
      "          0.1896,  0.9748],\n",
      "        [-1.2840, -0.7962, -0.6047, -0.8784,  0.7997,  0.8366,  5.6206, -0.3485,\n",
      "         -1.4491, -0.9492],\n",
      "        [-1.2943, -0.7917, -0.6074, -0.8710,  0.7887,  0.8474,  5.6175, -0.3549,\n",
      "         -1.4373, -0.9491]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    results = model(**inputs)\n",
    "    print([model.config.id2label[item.item()] for item in results.logits.argmax(axis=1)])\n",
    "    print(results.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
